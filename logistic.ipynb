{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2mResolved \u001b[1m29 packages\u001b[0m \u001b[2min 832ms\u001b[0m\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sympy \u001b[2m(6.0MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m setuptools \u001b[2m(1.0MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m networkx \u001b[2m(2.0MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(108.5MiB)\u001b[0m\n",
            " \u001b[36m\u001b[1mDownloaded\u001b[0m\u001b[39m setuptools\n",
            " \u001b[36m\u001b[1mDownloaded\u001b[0m\u001b[39m networkx\n",
            " \u001b[36m\u001b[1mDownloaded\u001b[0m\u001b[39m sympy\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(108.5MiB)\u001b[0m\n",
            " \u001b[36m\u001b[1mDownloaded\u001b[0m\u001b[39m torch\n",
            "\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 1m 46s\u001b[0m\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
            "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
            "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 8.33s\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.10.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv add torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "# Should print your GPU name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeD9M1F9UuRm",
        "outputId": "bfedb6d2-3bbb-48aa-b167-77ba385cea36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Introduction**\n",
        "\n",
        "1. Firstly we shall know the basics of logistic regression\n",
        "\n",
        "2. It primarily uses binary cross entropy as its loss function\n",
        "\n",
        "3. Here only w is set to requires gradient true, so its tracked to the auto grad tree to caluclate gradient\n",
        "\n",
        "4. Now here we are manually implementing the Adam optimizer function which works withh momentum and variance, it basically works with the moving averages of the gradients of the weigts\n",
        "\n",
        "5. Unlike Stochastic Gradient Descent optimizer we make sure we take the consideration of how fast or slow the gradient descant must take place\n",
        "\n",
        "6. It uses the first moment (momentum) and the second moment (uncentered variance) of the gradients to scale the updates.\n",
        "\n",
        "7. It follows the same for lr, learning rate, with adam optim we can change learning rate for everystep\n",
        "\n",
        "8. Here we just used decay to make sure its changed after 5 consecutive iterations where loss is not being decreased then we mutiply decay to get new lr, this is nothing but ReduceLR on Plateu ... This is generally done for validation loss\n",
        "\n",
        "9. So, in training we do adam for w, lr etc. in validation time we use reducelrplateu generally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.tensor([1.0,2.0,3.0,4.0],device=device)\n",
        "\n",
        "Y = torch.tensor([2.0,3.0,4.0,5.0],device=device)\n",
        "\n",
        "w = torch.tensor(0.0,requires_grad=True,device=device)\n",
        "\n",
        "b = torch.tensor(0.0,requires_grad=True,device =device)\n",
        "\n",
        "lr = 0.05\n",
        "beta1, beta2 = 0.9, 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# --- SCHEDULER SETTINGS ---\n",
        "best_loss = float('inf')\n",
        "patience = 5          # How many epochs to wait before dropping LR\n",
        "patience_counter = 0\n",
        "decay_factor = 0.5    # Cut LR in half\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "m, v, t = 0, 0, 0\n",
        "for epoch in range(100):\n",
        "\n",
        "  y_new = 1/(1 + torch.exp(w*X + b))\n",
        "\n",
        "  loss = -(Y*torch.log(y_new) + (1-Y)*torch.log(1-y_new)).mean()\n",
        "\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "        t += 1\n",
        "        g = w.grad\n",
        "        m = beta1 * m + (1 - beta1) * g\n",
        "        v = beta2 * v + (1 - beta2) * (g**2)\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "\n",
        "        w -= lr * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
        "\n",
        "\n",
        "  w.grad.zero_()\n",
        "  if loss.item() < best_loss:\n",
        "\n",
        "        best_loss = loss.item()\n",
        "\n",
        "        patience_counter = 0\n",
        "\n",
        "  else:\n",
        "    patience_counter +=1\n",
        "\n",
        "  if patience_counter >=patience:\n",
        "    lr = lr*decay_factor\n",
        "\n",
        "    patience_counter = 0\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item():.6f}, w {w.item():.4f}, current_lr {lr}\")\n",
        "\n",
        "print(f\"\\nFinal w: {w.item():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
