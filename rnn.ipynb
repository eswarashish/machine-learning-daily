{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5-qPkWaV6oNY"
      },
      "outputs": [],
      "source": [
        "# Lets build an RNN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(SimpleRNN,self).__init__()\n",
        "\n",
        "    # self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "    # self.relu = nn.ReLU()\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,num_layers=2)\n",
        "    self.fc2 = nn.Linear(hidden_size,output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out, _ = self.rnn(x)\n",
        "    return self.sigmoid(self.fc2(out[:, -1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WTwm-cyV6oIc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:1343: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  current = float(metrics)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/100], Loss: 0.6998, LR: 0.01\n",
            "Epoch [20/100], Loss: 0.0352, LR: 0.01\n",
            "Epoch [40/100], Loss: 0.0009, LR: 0.01\n",
            "Epoch [60/100], Loss: 0.0002, LR: 0.01\n",
            "Epoch [80/100], Loss: 0.0002, LR: 0.01\n",
            "Epoch [100/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [120/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [140/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [160/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [180/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [200/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [220/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [240/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [260/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [280/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [300/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [320/100], Loss: 0.0001, LR: 0.01\n",
            "Epoch [340/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [360/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [380/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [400/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [420/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [440/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [460/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [480/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [500/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [520/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [540/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [560/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [580/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [600/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [620/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [640/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [660/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [680/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [700/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [720/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [740/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [760/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [780/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [800/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [820/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [840/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [860/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [880/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [900/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [920/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [940/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [960/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [980/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1000/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1020/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1040/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1060/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1080/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1100/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1120/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1140/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1160/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1180/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1200/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1220/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1240/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1260/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1280/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1300/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1320/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1340/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1360/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1380/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1400/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1420/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1440/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1460/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1480/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1500/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1520/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1540/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1560/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1580/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1600/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1620/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1640/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1660/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1680/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1700/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1720/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1740/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1760/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1780/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1800/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1820/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1840/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1860/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1880/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1900/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1920/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1940/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1960/100], Loss: 0.0000, LR: 0.01\n",
            "Epoch [1980/100], Loss: 0.0000, LR: 0.01\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 1. DATA DIMENSIONS SETUP ---\n",
        "seq_len = 10    # Look back at the last 10 days\n",
        "features = 5    # OHLCV (Open, High, Low, Close, Volume)\n",
        "\n",
        "# Create dummy data: 32 samples, each is a 10-day window, 5 stats per day\n",
        "X = torch.randn(100, seq_len, features)\n",
        "# Target: 32 labels (e.g., price goes up=1 or down=0)\n",
        "Y = torch.randint(0, 2, (100, 1)).float()\n",
        "\n",
        "model = SimpleRNN(5,45,1)\n",
        "optimizer = optim.AdamW(model.parameters(),lr = 0.01)\n",
        "scheduler= optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.5,patience=15)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(2000):\n",
        "\n",
        "  outputs = model(X)\n",
        "  loss = criterion(outputs,Y)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  scheduler.step(loss)\n",
        "\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch [{epoch}/100], Loss: {loss.detach().item():.4f}, LR: {curr_lr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeFZLqS96oDx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SttqsZCTx4Qr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
